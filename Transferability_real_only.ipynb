{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6fe32e-5c7f-491e-bfe9-52b25c9d82ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "\n",
    "from data_module_F import *\n",
    "from model_module_F import *\n",
    "from feature_module_F import *\n",
    "from evaluation_module_F import *\n",
    "\n",
    "\n",
    "#from Resnet1D_builder import *\n",
    "from EfficientNet1D_builder import *\n",
    "from model_builder import *\n",
    "\n",
    "import sys, os\n",
    "\n",
    "tf.random.set_seed(155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f76617-2baf-4ac4-83ea-966670760220",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/net/home/dmederer/Transferability_efficient/processed_original_data.csv\" ## data path\n",
    "exp = \"comparison_training/\" ## experiment name \n",
    "route = '/net/home/dmederer/Transferability_efficient/'## store experiment's files name\n",
    "epochs = 300 ## num epoch\n",
    "lr = 0.0005 ### learning rate\n",
    "kind = 'efficientnet' ## type of model to train\n",
    "\n",
    "epochs = 300\n",
    "lr = 0.0005\n",
    "kind = 'efficientnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb77cb44-043a-433c-bb91-8bb5fb41f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## GPU RAM memory ##########\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 15*1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=1024*35)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f1055c-810d-469a-94b4-e7f1c0826e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOOSE the right file path\n",
    "\n",
    "path_g = route + '{}_spatialCV_real_only/'.format(exp)\n",
    "#path_g = route + '{}_spatialCV_rtm_only/'.format(exp)\n",
    "#path_g = route + '{}_spatialCV_low_var_test/'.format(exp)\n",
    "#path_g = route + '{}_spatialCV_frequency/'.format(exp)\n",
    "\n",
    "\n",
    "create_path(path_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8ddd6-a019-45ee-ab6e-bb194f7d9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/net/home/dmederer/Transferability_efficient/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d105629-1365-4848-8ed0-7b2c5e98aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load processed original data \n",
    "db, X, y = read_db(path ,sp=True)\n",
    "\n",
    "# Load processed rtm data\n",
    "db_rtm, X_rtm, y_rtm = read_db(\"processed_rtm_data.csv\" ,sp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94144b2f-5c34-4249-b530-0fe7ec938aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec0082-28d4-49c2-9c30-0a71be11fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_rtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9f723-6172-48f1-ba28-f5c7db3f6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gp in range(1, 43):\n",
    "    #if gp == 41: # to exclude certain datasets from training\n",
    "     #   continue\n",
    "    #### Split training and test data\n",
    "    db_test = db.groupby(\"dataset\").get_group(gp)\n",
    "    val_x, val_y, samp_w_val = data_prep('400', db_test, Traits, w_train = db_test.loc[:,:'Site'], multi= True)\n",
    "\n",
    "    db_train = db.drop(db_test.index, axis=0)\n",
    "\n",
    "    fgp = balanceData(db_train.reset_index(drop=True).loc[:,'Al content (mg/cmÂ²)':], \n",
    "                      db_train.loc[:,:'Site'].reset_index(drop=True), Traits, random_state=300)\n",
    "    \n",
    "    ### REMOVE THIS WHEN TRAINING ONLY REAL DATA\n",
    "    \n",
    "    ### Add a random subset of rtm data equal to the training fold size\n",
    "    #num_rows = len(fgp)\n",
    "    #db_rtm_subset = db_rtm.sample(n=num_rows, random_state=42)\n",
    "    \n",
    "    # Concatenate original data and rtm subset\n",
    "    #fgp = pd.concat([fgp, db_rtm_subset], axis=0, sort=False).reset_index(drop=True)\n",
    "    \n",
    "    # OPTIONAL: only use rtm data for training (rtm-only run)\n",
    "    #fgp = db_rtm_subset \n",
    "\n",
    "\n",
    "    ###\n",
    "    \n",
    "    train_x, train_y, samp_w_tr = data_prep('400', fgp, Traits, w_train = fgp.loc[:,:'Site'], multi= True)\n",
    "\n",
    "    scaler_list = save_scaler(train_y, save=True, dir_n = path_g, k = 'db{}'.format(gp))\n",
    "\n",
    "# LEAVE OUT FOR RTM DATA ONLY (keep train_ds and test_ds)\n",
    "    if(samp_w_tr is not None):\n",
    "        if (samp_w_tr.sum().sum() !=0):    \n",
    "            train_ds = dataset(train_x, train_y, pd.DataFrame(samp_w_tr), scaler_list, \n",
    "                               Traits, shuffle=True,augment=False)\n",
    "            test_ds = dataset(val_x, val_y, samp_w_val, scaler_list, Traits)\n",
    "    else:\n",
    "        train_ds = dataset(train_x, train_y, None, scaler_list, Traits, shuffle=True,augment=False) #!!!!!!\n",
    "        test_ds = dataset(val_x, val_y, None, scaler_list, Traits)\n",
    "\n",
    "    ##### Model definition  and training #######\n",
    "    input_shape = train_x.shape[1]\n",
    "    output_shape = train_y.shape[1]\n",
    "\n",
    "\n",
    "    EPOCHS = epochs \n",
    "    best_model = model_definition(input_shape, output_shape,lr = lr, kind= kind)\n",
    "\n",
    "    checkpoint_path = path_g + 'checkpoint_db{}'.format(gp)\n",
    "    create_path(checkpoint_path)\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_path + \"/epoch{epoch:02d}-val_root_mean_squared_error{val_root_mean_squared_error:.2f}.hdf5\",\n",
    "    save_weights_only=True,\n",
    "    monitor = 'val_root_mean_squared_error',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "    his = best_model.fit(train_ds,\n",
    "                    validation_data = test_ds,\n",
    "                    epochs = EPOCHS,\n",
    "                    verbose=1, callbacks = [model_checkpoint_callback])\n",
    "\n",
    "\n",
    "    val_acc_per_epoch = his.history['val_root_mean_squared_error']\n",
    "    best_epoch = val_acc_per_epoch.index(min(val_acc_per_epoch)) + 1    \n",
    "\n",
    "    path_trial = path_g + \"Model_db{}.json\".format(gp)\n",
    "    path_best = checkpoint_path + \"/epoch{0:02d}-val_root_mean_squared_error{1:.2f}.hdf5\".format(best_epoch,min(val_acc_per_epoch))\n",
    "    path_w = path_g + 'Trial_db{}_weights.h5'.format(gp)\n",
    "    save_model(best_model, path_trial, path_best, path_w)\n",
    "\n",
    "    pred = scaler_list.inverse_transform(best_model.predict(val_x))\n",
    "    pred_df = pd.DataFrame(pred, columns = val_y.columns+ ' Predictions')\n",
    "    pred_df.to_csv(path_g + 'Predictions_db{}.csv'.format(gp))\n",
    "\n",
    "    obs_pf = pd.DataFrame(val_y)\n",
    "    obs_pf.to_csv(path_g + 'Observations_db{}.csv'.format(gp))\n",
    "\n",
    "    test = all_scores(Traits,Traits,obs_pf, pred_df,None)\n",
    "    test.to_csv(path_g + 'Global_all_db{}.csv'.format(gp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462db145-9d40-400e-8916-fa4627dc2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FINAL STEP AFTER TRAINING ALL FOLDS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862cc12-48c0-4aad-a459-26a4763766c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da6e4d-f6b2-4d86-aba4-afffd04c1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path\n",
    "directory_path = '/net/home/dmederer/Transferability_efficient/comparison_training/_spatialCV_real_only/'\n",
    "#directory_path = '/net/home/dmederer/Transferability_efficient/comparison_training/_spatialCV_real_only_test/'\n",
    "#directory_path = '/net/home/dmederer/Transferability_efficient/comparison_training/_spatialCV_rtm_only/'\n",
    "#directory_path = '/net/home/dmederer/Transferability_efficient/comparison_training/_spatialCV_low_var/'\n",
    "#directory_path = '/net/home/dmederer/Transferability_efficient/comparison_training/_spatialCV_low_var_test/'\n",
    "#directory_path = '/net/home/dmederer/Transferability_efficient/comparison_training/_spatialCV_frequency/'\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c5ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define path_g to plot directory\n",
    "#path_g = /net/home/dmederer/Transferability_efficient/plot_folder\n",
    "\n",
    "path_g = route + 'plot_folder/'\n",
    "\n",
    "\n",
    "create_path(path_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ad3b9-e65b-4857-9afe-76d0aa9469d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and concatenate observations\n",
    "\n",
    "# Initialize an empty list to store the dataframes\n",
    "observation_dataframes = []\n",
    "\n",
    "# Loop through the numbered filenames\n",
    "for i in range(1, 43):\n",
    "    filename = f\"Observations_db{i}.csv\"  # Assuming the files have a \".csv\" extension\n",
    "    \n",
    "    try:\n",
    "        df = read_db(filename)  # Load the dataframe from the file\n",
    "        observation_dataframes.append(df)  # Append the dataframe to the list\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found. Skipping...\")\n",
    "\n",
    "# Concatenate the dataframes\n",
    "observation_combined_df = pd.concat(observation_dataframes, ignore_index=True)\n",
    "\n",
    "# Save the concatenated dataframe\n",
    "observation_combined_df.to_csv(path_g + \"Observations_combined_real_only.csv\", index=False)\n",
    "#observation_combined_df.to_csv(path_g + \"Observations_combined_real_only_test.csv\", index=False)\n",
    "#observation_combined_df.to_csv(path_g + \"Observations_combined_rtm_only.csv\", index=False)\n",
    "#observation_combined_df.to_csv(path_g + \"Observations_combined_low_var.csv\", index=False)\n",
    "#observation_combined_df.to_csv(path_g + \"Observations_combined_low_var_test_nodecay.csv\", index=False)\n",
    "#observation_combined_df.to_csv(path_g + \"Observations_combined_frequency.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79824d8-0a3f-4f05-9140-018f0b675928",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de324573-e10e-4d34-8bf8-87831c4b1fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and concatenate predictions\n",
    "\n",
    "# Initialize an empty list to store the dataframes\n",
    "prediction_dataframes = []\n",
    "\n",
    "# Loop through the numbered filenames\n",
    "for i in range(1, 43):\n",
    "    filename = f\"Predictions_db{i}.csv\"  # Assuming the files have a \".csv\" extension\n",
    "    \n",
    "    try:\n",
    "        df = read_db(filename)  # Load the dataframe from the file\n",
    "        prediction_dataframes.append(df)  # Append the dataframe to the list\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found. Skipping...\")\n",
    "\n",
    "# Concatenate the dataframes\n",
    "prediction_combined_df = pd.concat(prediction_dataframes, ignore_index=True)\n",
    "\n",
    "# Save the concatenated dataframe\n",
    "prediction_combined_df.to_csv(path_g + \"Predictions_combined_real_only.csv\", index=False)\n",
    "#prediction_combined_df.to_csv(path_g + \"Predictions_combined_real_only_test.csv\", index=False)\n",
    "#prediction_combined_df.to_csv(path_g + \"Predictions_combined_rtm_only.csv\", index=False)\n",
    "#prediction_combined_df.to_csv(path_g + \"Predictions_combined_low_var.csv\", index=False)\n",
    "#prediction_combined_df.to_csv(path_g + \"Predictions_combined_low_var_test_nodecay.csv\", index=False)\n",
    "#prediction_combined_df.to_csv(path_g + \"Predictions_combined_frequency.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4cd131-0b72-46a6-8416-367bda7e963a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95966737-0aac-4512-aac3-2bea16f1c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_combined_df.sort_values(by=[\"LMA (g/mÂ²)\"], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394ea41-fdd8-4ac2-8b1d-8676040f37c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = all_scores(Traits,Traits,observation_combined_df, prediction_combined_df,None)\n",
    "test.to_csv(path_g + 'Evaluation_transferability_combined_real_only.csv')\n",
    "#test.to_csv(path_g + 'Evaluation_transferability_combined_real_only_test.csv')\n",
    "#test.to_csv(path_g + 'Evaluation_transferability_combined_with_rtm_only.csv')\n",
    "#test.to_csv(path_g + 'Evaluation_transferability_combined_with_low_var.csv')\n",
    "#test.to_csv(path_g + 'Evaluation_transferability_combined_with_low_var_test_nodecay.csv')\n",
    "#test.to_csv(path_g + 'Evaluation_transferability_combined_with_frequency.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92aa992-c145-4087-84e5-8d3c0a48674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49d333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
